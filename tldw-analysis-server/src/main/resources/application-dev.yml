server:
  port: 18087

## **********************************************************
##   Elasticsearch Settings
## **********************************************************
elasticsearch:
  data:
    source:
      # es link
      host: http://172.30.6.128:9216
      # userName if elasticsearch open ssl
      username:
      # password if elasticsearch open ssl
      password:
      max_connect_total: 900
      max_connect_per_route: 300
      connection_request_timeout_millis: 8000
      socket_timeout_millis: 60000
      connect_timeout_millis: 8000
      prefix:

## **********************************************************
##    Spring Settings (Redis、HBase)
## **********************************************************
spring:
  #redis 配置
  redis:
    host: 172.30.6.31
    port: 6379
  data:
    #HBase 配置
    hbase:
      quorum: 172.30.6.31:2181
      rootDir: file:///hbase
      nodeParent: /hbase

## **********************************************************
##    Datasource Settings
## **********************************************************
enable:
  datasource:
    type: elasticsearch


## **********************************************************
##    资金战法配置
## **********************************************************
tactics:
  fund:
    ## 全部查询
    # 最大调单卡号数量限制
    max_adjust_card_count: 8000
    # 最大未调单卡号数量限制
    max_unadjusted_card_count: 20_0000
    # 针对查询的总数据量拆分
    per_total_split_count: 10000
    # 针对查询的总数据量拆分(继续拆分做数据批量查询)
    per_total_split_query_count: 2000
    ## 普通查询
    # group by size 的 临界值
    group_by_threshold: 100_0000
    # 每次查询数量(普通批量查询)
    per_query_count: 2000
    # 每次聚合查询数量(普通批量查询)
    per_agg_count: 5000
    # 分页查询上限
    pagination_threshold: 1000
    ## Excel导出阈值设置
    export:
      # 每个sheet页存储的记录数( 若查询的数据量超过,则开始拆分多sheet页)
      per_sheet_row_count: 20_0000
      # 每次向EXCEL写入的记录数(查询每页数据大小)
      per_write_row_count: 8_000
      # 导出Excel 的临界值 (超过这个值,不再导出)
      excel_export_threshold: 100_0000


## 快进快出调单卡号作为来源/中转/沉淀生成的分析结果
## 假设按照排序生成(流入金额、流出金额、流出日期) 共计 6种排序规则,每种规则若生成5w数据
## 如果满的话,调单卡号作为来源共计生成30w数据,依次类推 调单卡号作为来源、中转、沉淀情况都满足的话共计90w数据(然后基于这90w 需要继续过滤特征比和时间间隔等)
## 数据基本足够大(尽量做微调)
fast_inout:
  # eg. 调单卡号作为中转卡号情况, 我按流入金额排序取了1000条记录(拿到了来源-中转(借贷标志为进))
  # 还差中转-沉淀 (借贷标志为出) 这部分数据也可能会很多, 就是 unsortedThreshold
  per_unsorted_threshold: 10_0000
  # eg. 调单卡号作为中转卡号情况, 我按流入金额排序取了 sortThreshold 条记录(拿到了来源-中转(借贷标志为进)),这部分数据可能会很多
  per_sort_threshold: 2000
  per_agg_threshold: 5000
  two_hop_sort_threshold: 10_000

athena:
  gdb:
    uri: http://172.30.4.55:8089

## **********************************************************
##   Plugins Settings (插件其他相关配置 详见tldw-analysis-plugins/README.md 参考文档)
## **********************************************************
plugin:
  #插件启动方式 dev or prod
  runMode: prod
  #插件包放置位置 Linux环境修改为对应路径形式 eg：./plugins C:\Users\zhangkehou\IdeaProjects\dev_jz_2.6.4_version_upgrade\santaizi\tldw-analysis-server\lib
  pluginPath: C:\\Users\\zhangkehou\\IdeaProjects\\dev_jz_2.6.4_version_upgrade\\santaizi\\tldw-analysis-server\\lib
  #  pluginPath: ./tldw-analysis-plugins
  #插件包配置文件路径
  #  pluginConfigFilePath: /opt/tldw/analysis/tldw-analysis-plugins
  pluginConfigFilePath:

